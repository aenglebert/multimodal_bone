_target_: transformers.TrainingArguments
output_dir: "checkpoints/"
per_device_train_batch_size: 8
per_device_eval_batch_size: 4
gradient_accumulation_steps: 10
learning_rate: 0.000002
warmup_steps: 1000
weight_decay: 0.01
optim: lion_8bit
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-6
save_steps: 1000
save_total_limit: 2
logging_dir: "output/logs"
logging_strategy: "steps"
logging_steps: ${logging_steps}
fp16: True
dataloader_num_workers: ${num_proc}
run_name: "vlp_lion_8bit_optim"
report_to: "wandb"
eval_steps: 5000
evaluation_strategy: "steps"
prediction_loss_only: True