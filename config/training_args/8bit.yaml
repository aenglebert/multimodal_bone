_target_: transformers.TrainingArguments
output_dir: "checkpoints/"
num_train_epochs: 1
per_device_train_batch_size: 8
per_device_eval_batch_size: 4
gradient_accumulation_steps: 10
learning_rate: 0.0002
warmup_steps: 10
weight_decay: 0.01
optim: adamw_bnb_8bit
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1e-6
save_steps: 1000
save_total_limit: 2
logging_dir: "output/logs"
logging_strategy: "steps"
logging_steps: ${logging_steps}
fp16: True
dataloader_num_workers: ${num_proc}
run_name: "vlp_8bit_optim"
report_to: "wandb"